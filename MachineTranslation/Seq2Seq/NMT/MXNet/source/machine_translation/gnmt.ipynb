{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training GNMT on IWSLT 2015 Dataset\n",
    "\n",
    "In this notebook, we are going to train Google NMT on IWSLT 2015 English-Vietnamese\n",
    "Dataset. The building process includes four key steps:\n",
    "\n",
    "1. Load and preprocess the dataset\n",
    "\n",
    "2. Create a sampler and `DataLoader`\n",
    "\n",
    "3. Build the actual model\n",
    "\n",
    "4. Write the training algorithm\n",
    "\n",
    "This tutorial will guide you through each of the steps and explain briefly how each works. Please remember to click the download button at the top of the page to download the necessary files to follow this tutorial.\n",
    "\n",
    "## Setup\n",
    "\n",
    "Firstly, we need to setup the environment and import the necessary modules. For this tutorial, a GPU is highly important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import argparse\n",
    "import time\n",
    "import random\n",
    "import os\n",
    "import io\n",
    "import logging\n",
    "import numpy as np\n",
    "import mxnet as mx\n",
    "from mxnet import gluon\n",
    "import gluonnlp as nlp\n",
    "import nmt\n",
    "nlp.utils.check_version('0.7.0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to specify the hyperparameters for the dataset, the model, and for training and testing time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Logs will be saved to gnmt_en_vi_u512/<ipython-input-2-4699ac3a1bfb>.log\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'gnmt_en_vi_u512'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(100)\n",
    "random.seed(100)\n",
    "mx.random.seed(10000)\n",
    "ctx = mx.gpu(0)\n",
    "\n",
    "# parameters for dataset\n",
    "dataset = 'IWSLT2015'\n",
    "src_lang, tgt_lang = 'en', 'vi'\n",
    "src_max_len, tgt_max_len = 50, 50\n",
    "\n",
    "# parameters for model\n",
    "num_hidden = 512\n",
    "num_layers = 2\n",
    "num_bi_layers = 1\n",
    "dropout = 0.2\n",
    "\n",
    "# parameters for training\n",
    "batch_size, test_batch_size = 128, 32\n",
    "num_buckets = 5\n",
    "epochs = 1\n",
    "clip = 5\n",
    "lr = 0.001\n",
    "lr_update_factor = 0.5\n",
    "log_interval = 10\n",
    "save_dir = 'gnmt_en_vi_u512'\n",
    "\n",
    "#parameters for testing\n",
    "beam_size = 10\n",
    "lp_alpha = 1.0\n",
    "lp_k = 5\n",
    "\n",
    "nmt.utils.logging_config(save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and processing the dataset\n",
    "\n",
    "The following shows how to process the dataset and cache the processed dataset\n",
    "for future use. The processing steps include the following:\n",
    "\n",
    "1. Clipping the source and target sequences\n",
    "2. Splitting the string input to a list of tokens\n",
    "3. Mapping the string token onto its integer index in the vocabulary\n",
    "4. Appending the end-of-sentence (EOS) token to source sentence and adding BOS and EOS tokens to the target sentence\n",
    "\n",
    "\n",
    "Firstly, we load and cache the dataset with the two helper functions `cache_dataset` and `load_cached_dataset`. The functions are straightforward and well commented so no further explanation will be given."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cache_dataset(dataset, prefix):\n",
    "    \"\"\"Cache the processed npy dataset  the dataset into an npz file\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset : gluon.data.SimpleDataset\n",
    "    file_path : str\n",
    "    \"\"\"\n",
    "    if not os.path.exists(nmt._constants.CACHE_PATH):\n",
    "        os.makedirs(nmt._constants.CACHE_PATH)\n",
    "    src_data = np.concatenate([e[0] for e in dataset])\n",
    "    tgt_data = np.concatenate([e[1] for e in dataset])\n",
    "    src_cumlen = np.cumsum([0]+[len(e[0]) for e in dataset])\n",
    "    tgt_cumlen = np.cumsum([0]+[len(e[1]) for e in dataset])\n",
    "    np.savez(os.path.join(nmt._constants.CACHE_PATH, prefix + '.npz'),\n",
    "             src_data=src_data, tgt_data=tgt_data,\n",
    "             src_cumlen=src_cumlen, tgt_cumlen=tgt_cumlen)\n",
    "\n",
    "\n",
    "def load_cached_dataset(prefix):\n",
    "    cached_file_path = os.path.join(nmt._constants.CACHE_PATH, prefix + '.npz')\n",
    "    if os.path.exists(cached_file_path):\n",
    "        print('Load cached data from {}'.format(cached_file_path))\n",
    "        npz_data = np.load(cached_file_path)\n",
    "        src_data, tgt_data, src_cumlen, tgt_cumlen = [npz_data[n] for n in\n",
    "                ['src_data', 'tgt_data', 'src_cumlen', 'tgt_cumlen']]\n",
    "        src_data = np.array([src_data[low:high] for low, high in zip(src_cumlen[:-1], src_cumlen[1:])])\n",
    "        tgt_data = np.array([tgt_data[low:high] for low, high in zip(tgt_cumlen[:-1], tgt_cumlen[1:])])\n",
    "        return gluon.data.ArrayDataset(np.array(src_data), np.array(tgt_data))\n",
    "    else:\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we write the class `TrainValDataTransform` to have easy access to transforming and clipping the source and target sentences. This class also adds the EOS and BOS tokens for cleaner data. Please refer to the comments in the code for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainValDataTransform(object):\n",
    "    \"\"\"Transform the machine translation dataset.\n",
    "\n",
    "    Clip source and the target sentences to the maximum length. For the source sentence, append the\n",
    "    EOS. For the target sentence, append BOS and EOS.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    src_vocab : Vocab\n",
    "    tgt_vocab : Vocab\n",
    "    src_max_len : int\n",
    "    tgt_max_len : int\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, src_vocab, tgt_vocab, src_max_len, tgt_max_len):\n",
    "        # On initialization of the class, we set the class variables\n",
    "        self._src_vocab = src_vocab\n",
    "        self._tgt_vocab = tgt_vocab\n",
    "        self._src_max_len = src_max_len\n",
    "        self._tgt_max_len = tgt_max_len\n",
    "\n",
    "    def __call__(self, src, tgt):\n",
    "        # On actual calling of the class, we perform the clipping then the appending of the EOS and BOS tokens.\n",
    "        if self._src_max_len > 0:\n",
    "            src_sentence = self._src_vocab[src.split()[:self._src_max_len]]\n",
    "        else:\n",
    "            src_sentence = self._src_vocab[src.split()]\n",
    "        if self._tgt_max_len > 0:\n",
    "            tgt_sentence = self._tgt_vocab[tgt.split()[:self._tgt_max_len]]\n",
    "        else:\n",
    "            tgt_sentence = self._tgt_vocab[tgt.split()]\n",
    "        src_sentence.append(self._src_vocab[self._src_vocab.eos_token])\n",
    "        tgt_sentence.insert(0, self._tgt_vocab[self._tgt_vocab.bos_token])\n",
    "        tgt_sentence.append(self._tgt_vocab[self._tgt_vocab.eos_token])\n",
    "        src_npy = np.array(src_sentence, dtype=np.int32)\n",
    "        tgt_npy = np.array(tgt_sentence, dtype=np.int32)\n",
    "        return src_npy, tgt_npy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We leverage the class written above to create a helper function that processes the dataset in very few lines of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataset(dataset, src_vocab, tgt_vocab, src_max_len=-1, tgt_max_len=-1):\n",
    "    start = time.time()\n",
    "    dataset_processed = dataset.transform(TrainValDataTransform(src_vocab, tgt_vocab,\n",
    "                                                                src_max_len,\n",
    "                                                                tgt_max_len), lazy=False)\n",
    "    end = time.time()\n",
    "    print('Processing time spent: {}'.format(end - start))\n",
    "    return dataset_processed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define a function `load_translation_data` that combines all the above steps to load the data, check if it's been processed, and if not, process the data. The method returns all of the required data for training, validating, and testing our model. Please refer to the comments in the code for more information on what each piece does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_translation_data(dataset, src_lang='en', tgt_lang='vi'):\n",
    "    \"\"\"Load translation dataset\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset : str\n",
    "    src_lang : str, default 'en'\n",
    "    tgt_lang : str, default 'vi'\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    data_train_processed : Dataset\n",
    "        The preprocessed training sentence pairs\n",
    "    data_val_processed : Dataset\n",
    "        The preprocessed validation sentence pairs\n",
    "    data_test_processed : Dataset\n",
    "        The preprocessed test sentence pairs\n",
    "    val_tgt_sentences : list\n",
    "        The target sentences in the validation set\n",
    "    test_tgt_sentences : list\n",
    "        The target sentences in the test set\n",
    "    src_vocab : Vocab\n",
    "        Vocabulary of the source language\n",
    "    tgt_vocab : Vocab\n",
    "        Vocabulary of the target language\n",
    "    \"\"\"\n",
    "    common_prefix = 'IWSLT2015_{}_{}_{}_{}'.format(src_lang, tgt_lang,\n",
    "                                                   src_max_len, tgt_max_len)\n",
    "\n",
    "    # Load the three datasets from files\n",
    "    data_train = nlp.data.IWSLT2015('train', src_lang=src_lang, tgt_lang=tgt_lang)\n",
    "    data_val = nlp.data.IWSLT2015('val', src_lang=src_lang, tgt_lang=tgt_lang)\n",
    "    data_test = nlp.data.IWSLT2015('test', src_lang=src_lang, tgt_lang=tgt_lang)\n",
    "    src_vocab, tgt_vocab = data_train.src_vocab, data_train.tgt_vocab\n",
    "    data_train_processed = load_cached_dataset(common_prefix + '_train')\n",
    "\n",
    "    # Check if each dataset has been processed or not, and if not, process and cache them.\n",
    "    if not data_train_processed:\n",
    "        data_train_processed = process_dataset(data_train, src_vocab, tgt_vocab,\n",
    "                                               src_max_len, tgt_max_len)\n",
    "        cache_dataset(data_train_processed, common_prefix + '_train')\n",
    "    data_val_processed = load_cached_dataset(common_prefix + '_val')\n",
    "    if not data_val_processed:\n",
    "        data_val_processed = process_dataset(data_val, src_vocab, tgt_vocab)\n",
    "        cache_dataset(data_val_processed, common_prefix + '_val')\n",
    "    data_test_processed = load_cached_dataset(common_prefix + '_test')\n",
    "    if not data_test_processed:\n",
    "        data_test_processed = process_dataset(data_test, src_vocab, tgt_vocab)\n",
    "        cache_dataset(data_test_processed, common_prefix + '_test')\n",
    "\n",
    "    # Pull out the target sentences for both test and validation\n",
    "    fetch_tgt_sentence = lambda src, tgt: tgt.split()\n",
    "    val_tgt_sentences = list(data_val.transform(fetch_tgt_sentence))\n",
    "    test_tgt_sentences = list(data_test.transform(fetch_tgt_sentence))\n",
    "\n",
    "    # Return all of the necessary pieces we can extract from the data for training our model\n",
    "    return data_train_processed, data_val_processed, data_test_processed, \\\n",
    "           val_tgt_sentences, test_tgt_sentences, src_vocab, tgt_vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a last helper function `get_data_lengths` to get the length of the datasets, again, for simplified cleaner code later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_lengths(dataset):\n",
    "    return list(dataset.transform(lambda srg, tgt: (len(srg), len(tgt))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And for the last step of processing, we leverage all of our helper functions to keep the code concise and to these 15-20 lines for use in our main. This does all of the aforementioned processing along with storing the necessary information in memory for training our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading /root/.mxnet/datasets/iwslt2015/iwslt15.zip from https://apache-mxnet.s3-accelerate.dualstack.amazonaws.com/gluon/dataset/iwslt2015/iwslt15.zip...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing time spent: 5.237854719161987\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing time spent: 0.05798935890197754\n",
      "Processing time spent: 0.052637577056884766\n"
     ]
    }
   ],
   "source": [
    "data_train, data_val, data_test, val_tgt_sentences, test_tgt_sentences, src_vocab, tgt_vocab\\\n",
    "    = load_translation_data(dataset=dataset, src_lang=src_lang, tgt_lang=tgt_lang)\n",
    "data_train_lengths = get_data_lengths(data_train)\n",
    "data_val_lengths = get_data_lengths(data_val)\n",
    "data_test_lengths = get_data_lengths(data_test)\n",
    "\n",
    "with io.open(os.path.join(save_dir, 'val_gt.txt'), 'w', encoding='utf-8') as of:\n",
    "    for ele in val_tgt_sentences:\n",
    "        of.write(' '.join(ele) + '\\n')\n",
    "\n",
    "with io.open(os.path.join(save_dir, 'test_gt.txt'), 'w', encoding='utf-8') as of:\n",
    "    for ele in test_tgt_sentences:\n",
    "        of.write(' '.join(ele) + '\\n')\n",
    "\n",
    "\n",
    "data_train = data_train.transform(lambda src, tgt: (src, tgt, len(src), len(tgt)), lazy=False)\n",
    "data_val = gluon.data.SimpleDataset([(ele[0], ele[1], len(ele[0]), len(ele[1]), i)\n",
    "                                     for i, ele in enumerate(data_val)])\n",
    "data_test = gluon.data.SimpleDataset([(ele[0], ele[1], len(ele[0]), len(ele[1]), i)\n",
    "                                      for i, ele in enumerate(data_test)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampler and `DataLoader` construction\n",
    "\n",
    "Now, we have obtained and stored all of the relevant data information. The next step\n",
    "is to construct the sampler and `DataLoader`. The first step is to use the `batchify`\n",
    "function, which pads and stacks sequences to form mini-batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batchify_fn = nlp.data.batchify.Tuple(nlp.data.batchify.Pad(pad_val=0),\n",
    "                                            nlp.data.batchify.Pad(pad_val=0),\n",
    "                                            nlp.data.batchify.Stack(dtype='float32'),\n",
    "                                            nlp.data.batchify.Stack(dtype='float32'))\n",
    "test_batchify_fn = nlp.data.batchify.Tuple(nlp.data.batchify.Pad(pad_val=0),\n",
    "                                           nlp.data.batchify.Pad(pad_val=0),\n",
    "                                           nlp.data.batchify.Stack(dtype='float32'),\n",
    "                                           nlp.data.batchify.Stack(dtype='float32'),\n",
    "                                           nlp.data.batchify.Stack())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then construct bucketing samplers, which generate batches by grouping\n",
    "sequences with similar lengths. Here, the bucketing scheme is empirically determined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-05 22:02:54,295 - root - Train Batch Sampler:\n",
      "FixedBucketSampler:\n",
      "  sample_num=133166, batch_num=1043\n",
      "  key=[(9, 10), (16, 17), (26, 27), (37, 38), (51, 52)]\n",
      "  cnt=[11414, 34897, 37760, 23480, 25615]\n",
      "  batch_size=[128, 128, 128, 128, 128]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-05 22:02:54,298 - root - Valid Batch Sampler:\n",
      "FixedBucketSampler:\n",
      "  sample_num=1553, batch_num=52\n",
      "  key=[(22, 28), (40, 52), (58, 76), (76, 100), (94, 124)]\n",
      "  cnt=[1037, 432, 67, 10, 7]\n",
      "  batch_size=[32, 32, 32, 32, 32]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-05 22:02:54,300 - root - Test Batch Sampler:\n",
      "FixedBucketSampler:\n",
      "  sample_num=1268, batch_num=42\n",
      "  key=[(23, 29), (43, 53), (63, 77), (83, 101), (103, 125)]\n",
      "  cnt=[770, 381, 84, 26, 7]\n",
      "  batch_size=[32, 32, 32, 32, 32]\n"
     ]
    }
   ],
   "source": [
    "bucket_scheme = nlp.data.ExpWidthBucket(bucket_len_step=1.2)\n",
    "train_batch_sampler = nlp.data.FixedBucketSampler(lengths=data_train_lengths,\n",
    "                                                  batch_size=batch_size,\n",
    "                                                  num_buckets=num_buckets,\n",
    "                                                  shuffle=True,\n",
    "                                                  bucket_scheme=bucket_scheme)\n",
    "logging.info('Train Batch Sampler:\\n{}'.format(train_batch_sampler.stats()))\n",
    "val_batch_sampler = nlp.data.FixedBucketSampler(lengths=data_val_lengths,\n",
    "                                                batch_size=test_batch_size,\n",
    "                                                num_buckets=num_buckets,\n",
    "                                                shuffle=False)\n",
    "logging.info('Valid Batch Sampler:\\n{}'.format(val_batch_sampler.stats()))\n",
    "test_batch_sampler = nlp.data.FixedBucketSampler(lengths=data_test_lengths,\n",
    "                                                 batch_size=test_batch_size,\n",
    "                                                 num_buckets=num_buckets,\n",
    "                                                 shuffle=False)\n",
    "logging.info('Test Batch Sampler:\\n{}'.format(test_batch_sampler.stats()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the samplers, we can create a `DataLoader`, which is iterable. This simply is a data construct (an iterator) that can feed the model batches at a time. For more information refer to [this](https://mxnet.incubator.apache.org/versions/master/tutorials/gluon/datasets.html) page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_loader = gluon.data.DataLoader(data_train,\n",
    "                                          batch_sampler=train_batch_sampler,\n",
    "                                          batchify_fn=train_batchify_fn,\n",
    "                                          num_workers=4)\n",
    "val_data_loader = gluon.data.DataLoader(data_val,\n",
    "                                        batch_sampler=val_batch_sampler,\n",
    "                                        batchify_fn=test_batchify_fn,\n",
    "                                        num_workers=4)\n",
    "test_data_loader = gluon.data.DataLoader(data_test,\n",
    "                                         batch_sampler=test_batch_sampler,\n",
    "                                         batchify_fn=test_batchify_fn,\n",
    "                                         num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the GNMT model\n",
    "\n",
    "After obtaining the DataLoader, we can finally build the model. The GNMT encoder and decoder\n",
    "can be easily constructed by calling `get_gnmt_encoder_decoder` function. Then, we\n",
    "feed the encoder and decoder to the `NMTModel` to construct the GNMT model.\n",
    "\n",
    "`model.hybridize` allows computation to be done using the symbolic backend. To understand what it means to be \"hybridized,\" please refer to [this](https://mxnet.incubator.apache.org/versions/master/tutorials/gluon/hybrid.html) page on MXNet hybridization and its advantages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-05 22:02:58,584 - root - NMTModel(\n",
      "  (one_step_ahead_decoder): GNMTOneStepDecoder(\n",
      "    (dropout_layer): Dropout(p = 0.2, axes=())\n",
      "    (rnn_cells): HybridSequential(\n",
      "      (0): LSTMCell(None -> 2048)\n",
      "      (1): LSTMCell(None -> 2048)\n",
      "    )\n",
      "    (attention_cell): DotProductAttentionCell(\n",
      "      (_proj_query): Dense(None -> 512, linear)\n",
      "      (_dropout_layer): Dropout(p = 0.0, axes=())\n",
      "    )\n",
      "  )\n",
      "  (tgt_proj): Dense(None -> 7709, linear)\n",
      "  (src_embed): HybridSequential(\n",
      "    (0): Embedding(17191 -> 512, float32)\n",
      "    (1): Dropout(p = 0.0, axes=())\n",
      "  )\n",
      "  (encoder): GNMTEncoder(\n",
      "    (dropout_layer): Dropout(p = 0.2, axes=())\n",
      "    (rnn_cells): HybridSequential(\n",
      "      (0): BidirectionalCell(forward=LSTMCell(None -> 2048), backward=LSTMCell(None -> 2048))\n",
      "      (1): LSTMCell(None -> 2048)\n",
      "    )\n",
      "  )\n",
      "  (tgt_embed): HybridSequential(\n",
      "    (0): Embedding(7709 -> 512, float32)\n",
      "    (1): Dropout(p = 0.0, axes=())\n",
      "  )\n",
      "  (decoder): GNMTDecoder(\n",
      "    (dropout_layer): Dropout(p = 0.2, axes=())\n",
      "    (rnn_cells): HybridSequential(\n",
      "      (0): LSTMCell(None -> 2048)\n",
      "      (1): LSTMCell(None -> 2048)\n",
      "    )\n",
      "    (attention_cell): DotProductAttentionCell(\n",
      "      (_proj_query): Dense(None -> 512, linear)\n",
      "      (_dropout_layer): Dropout(p = 0.0, axes=())\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "encoder, decoder, one_step_ahead_decoder = nmt.gnmt.get_gnmt_encoder_decoder(\n",
    "    hidden_size=num_hidden, dropout=dropout, num_layers=num_layers,\n",
    "    num_bi_layers=num_bi_layers)\n",
    "model = nlp.model.translation.NMTModel(src_vocab=src_vocab, tgt_vocab=tgt_vocab, encoder=encoder,\n",
    "                                       decoder=decoder, one_step_ahead_decoder=one_step_ahead_decoder,\n",
    "                                       embed_size=num_hidden, prefix='gnmt_')\n",
    "model.initialize(init=mx.init.Uniform(0.1), ctx=ctx)\n",
    "static_alloc = True\n",
    "model.hybridize(static_alloc=static_alloc)\n",
    "logging.info(model)\n",
    "\n",
    "# Due to the paddings, we need to mask out the losses corresponding to padding tokens.\n",
    "loss_function = nlp.loss.MaskedSoftmaxCELoss()\n",
    "loss_function.hybridize(static_alloc=static_alloc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we build the `BeamSearchTranslator` and define a predetermined `BeamSearchScorer` as the heuristical mechanism for the search. For more information on Beam Search and its applications to NLP, check [here](https://en.wikipedia.org/wiki/Beam_search)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-05 22:02:58,595 - root - Use beam_size=10, alpha=1.0, K=5\n"
     ]
    }
   ],
   "source": [
    "translator = nmt.translation.BeamSearchTranslator(model=model, beam_size=beam_size,\n",
    "                                                  scorer=nlp.model.BeamSearchScorer(alpha=lp_alpha,\n",
    "                                                                                    K=lp_k),\n",
    "                                                  max_length=tgt_max_len + 100)\n",
    "logging.info('Use beam_size={}, alpha={}, K={}'.format(beam_size, lp_alpha, lp_k))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the evaluation function as shown in the code block below. The `evaluate` function uses the beam\n",
    "search translator to generate outputs for the validation and testing datasets. Please refer to the comments in the code for more information on what each piece does. In addition, we add the `write_sentences` helper method to easily output the sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(data_loader):\n",
    "    \"\"\"Evaluate given the data loader\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data_loader : gluon.data.DataLoader\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    avg_loss : float\n",
    "        Average loss\n",
    "    real_translation_out : list of list of str\n",
    "        The translation output\n",
    "    \"\"\"\n",
    "    translation_out = []\n",
    "    all_inst_ids = []\n",
    "    avg_loss_denom = 0\n",
    "    avg_loss = 0.0\n",
    "\n",
    "    for _, (src_seq, tgt_seq, src_valid_length, tgt_valid_length, inst_ids) \\\n",
    "            in enumerate(data_loader):\n",
    "        src_seq = src_seq.as_in_context(ctx)\n",
    "        tgt_seq = tgt_seq.as_in_context(ctx)\n",
    "        src_valid_length = src_valid_length.as_in_context(ctx)\n",
    "        tgt_valid_length = tgt_valid_length.as_in_context(ctx)\n",
    "\n",
    "        # Calculate Loss\n",
    "        out, _ = model(src_seq, tgt_seq[:, :-1], src_valid_length, tgt_valid_length - 1)\n",
    "        loss = loss_function(out, tgt_seq[:, 1:], tgt_valid_length - 1).mean().asscalar()\n",
    "        all_inst_ids.extend(inst_ids.asnumpy().astype(np.int32).tolist())\n",
    "        avg_loss += loss * (tgt_seq.shape[1] - 1)\n",
    "        avg_loss_denom += (tgt_seq.shape[1] - 1)\n",
    "\n",
    "        # Translate the sequences and score them\n",
    "        samples, _, sample_valid_length =\\\n",
    "            translator.translate(src_seq=src_seq, src_valid_length=src_valid_length)\n",
    "        max_score_sample = samples[:, 0, :].asnumpy()\n",
    "        sample_valid_length = sample_valid_length[:, 0].asnumpy()\n",
    "\n",
    "        # Iterate through the tokens and stitch the tokens together for the sentence\n",
    "        for i in range(max_score_sample.shape[0]):\n",
    "            translation_out.append(\n",
    "                [tgt_vocab.idx_to_token[ele] for ele in\n",
    "                 max_score_sample[i][1:(sample_valid_length[i] - 1)]])\n",
    "\n",
    "    # Calculate the average loss and initialize a None-filled translation list\n",
    "    avg_loss = avg_loss / avg_loss_denom\n",
    "    real_translation_out = [None for _ in range(len(all_inst_ids))]\n",
    "\n",
    "    # Combine all the words/tokens into a sentence for the final translation\n",
    "    for ind, sentence in zip(all_inst_ids, translation_out):\n",
    "        real_translation_out[ind] = sentence\n",
    "\n",
    "    # Return the loss and the translation\n",
    "    return avg_loss, real_translation_out\n",
    "\n",
    "\n",
    "def write_sentences(sentences, file_path):\n",
    "    with io.open(file_path, 'w', encoding='utf-8') as of:\n",
    "        for sent in sentences:\n",
    "            of.write(' '.join(sent) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Before entering the training stage, we need to create a trainer for updating the\n",
    "parameters based on the loss. In the following example, we create a trainer that uses the ADAM\n",
    "optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = gluon.Trainer(model.collect_params(), 'adam', {'learning_rate': lr})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then write the training loop. During the training, we evaluate on the validation and testing datasets every epoch, and record the\n",
    "parameters that give the highest [Bilingual Evaluation Understudy Score (BLEU)](https://www.aclweb.org/anthology/P02-1040.pdf) score on the validation dataset. Before\n",
    "performing forward and backward computation, we first use the `as_in_context` function to copy\n",
    "the mini-batch to the GPU. The statement `with mx.autograd.record()` tells Gluon's\n",
    "backend to compute the gradients for the part inside the block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-05 22:03:02,983 - root - [Epoch 0 Batch 10/1043] loss=7.7636, ppl=2353.4034, gnorm=1.5034, throughput=12.50K wps, wc=54.27K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-05 22:03:04,577 - root - [Epoch 0 Batch 20/1043] loss=6.3430, ppl=568.5267, gnorm=1.5107, throughput=31.52K wps, wc=50.20K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-05 22:03:06,703 - root - [Epoch 0 Batch 30/1043] loss=6.3594, ppl=577.8955, gnorm=0.7529, throughput=31.89K wps, wc=67.78K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-05 22:03:08,678 - root - [Epoch 0 Batch 40/1043] loss=6.1655, ppl=476.0433, gnorm=0.5288, throughput=32.01K wps, wc=63.19K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-05 22:03:10,671 - root - [Epoch 0 Batch 50/1043] loss=6.1833, ppl=484.5670, gnorm=0.5388, throughput=31.09K wps, wc=61.93K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-05 22:03:12,562 - root - [Epoch 0 Batch 60/1043] loss=6.0925, ppl=442.5096, gnorm=0.7216, throughput=31.31K wps, wc=59.19K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-05 22:03:14,867 - root - [Epoch 0 Batch 70/1043] loss=6.1401, ppl=464.0799, gnorm=0.4698, throughput=31.69K wps, wc=72.99K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-05 22:03:16,940 - root - [Epoch 0 Batch 80/1043] loss=6.0602, ppl=428.4726, gnorm=0.3620, throughput=31.16K wps, wc=64.58K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-05 22:03:18,682 - root - [Epoch 0 Batch 90/1043] loss=5.9279, ppl=375.3819, gnorm=0.3948, throughput=30.46K wps, wc=53.02K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-05 22:03:20,559 - root - [Epoch 0 Batch 100/1043] loss=5.8780, ppl=357.0978, gnorm=0.4438, throughput=31.68K wps, wc=59.42K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-05 22:03:22,619 - root - [Epoch 0 Batch 110/1043] loss=5.8629, ppl=351.7384, gnorm=0.3592, throughput=31.81K wps, wc=65.50K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-05 22:03:24,533 - root - [Epoch 0 Batch 120/1043] loss=5.8458, ppl=345.7877, gnorm=0.3202, throughput=30.55K wps, wc=58.43K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-05 22:03:26,615 - root - [Epoch 0 Batch 130/1043] loss=5.8980, ppl=364.3204, gnorm=0.3507, throughput=28.54K wps, wc=59.39K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-05 22:03:28,640 - root - [Epoch 0 Batch 140/1043] loss=5.8481, ppl=346.5848, gnorm=0.2840, throughput=30.25K wps, wc=61.18K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-05 22:03:30,518 - root - [Epoch 0 Batch 150/1043] loss=5.7623, ppl=318.0699, gnorm=0.2929, throughput=30.01K wps, wc=56.34K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-05 22:03:32,472 - root - [Epoch 0 Batch 160/1043] loss=5.6958, ppl=297.6206, gnorm=0.3834, throughput=29.68K wps, wc=57.93K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-05 22:03:34,570 - root - [Epoch 0 Batch 170/1043] loss=5.7127, ppl=302.6945, gnorm=0.3042, throughput=30.72K wps, wc=64.42K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-05 22:03:36,064 - root - [Epoch 0 Batch 180/1043] loss=5.4089, ppl=223.3751, gnorm=0.3627, throughput=29.69K wps, wc=44.31K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-05 22:03:38,150 - root - [Epoch 0 Batch 190/1043] loss=5.5708, ppl=262.6376, gnorm=0.3558, throughput=29.97K wps, wc=62.45K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-05 22:03:39,976 - root - [Epoch 0 Batch 200/1043] loss=5.4712, ppl=237.7463, gnorm=0.3522, throughput=29.73K wps, wc=54.24K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-05 22:03:41,753 - root - [Epoch 0 Batch 210/1043] loss=5.2892, ppl=198.1918, gnorm=0.4377, throughput=29.67K wps, wc=52.68K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-05 22:03:43,446 - root - [Epoch 0 Batch 220/1043] loss=5.2762, ppl=195.6183, gnorm=0.3712, throughput=29.83K wps, wc=50.47K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-05 22:03:45,446 - root - [Epoch 0 Batch 230/1043] loss=5.2838, ppl=197.1127, gnorm=0.4389, throughput=30.85K wps, wc=61.65K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-05 22:03:47,364 - root - [Epoch 0 Batch 240/1043] loss=5.2746, ppl=195.3190, gnorm=0.3568, throughput=30.44K wps, wc=58.32K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-05 22:03:49,685 - root - [Epoch 0 Batch 250/1043] loss=5.4011, ppl=221.6552, gnorm=0.2991, throughput=30.54K wps, wc=70.84K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-05 22:03:51,672 - root - [Epoch 0 Batch 260/1043] loss=5.2482, ppl=190.2232, gnorm=0.3616, throughput=30.34K wps, wc=60.22K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-05 22:03:54,056 - root - [Epoch 0 Batch 270/1043] loss=5.3489, ppl=210.3777, gnorm=0.2841, throughput=30.63K wps, wc=72.96K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-05 22:03:56,115 - root - [Epoch 0 Batch 280/1043] loss=5.2222, ppl=185.3480, gnorm=0.2746, throughput=29.55K wps, wc=60.80K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-05 22:03:57,694 - root - [Epoch 0 Batch 290/1043] loss=4.9363, ppl=139.2498, gnorm=0.3440, throughput=29.03K wps, wc=45.79K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-05 22:03:59,645 - root - [Epoch 0 Batch 300/1043] loss=5.0548, ppl=156.7787, gnorm=0.3332, throughput=30.28K wps, wc=59.05K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-05 22:04:01,720 - root - [Epoch 0 Batch 310/1043] loss=5.0704, ppl=159.2324, gnorm=0.3007, throughput=29.70K wps, wc=61.58K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-05 22:04:03,523 - root - [Epoch 0 Batch 320/1043] loss=4.9469, ppl=140.7447, gnorm=0.3110, throughput=29.46K wps, wc=53.10K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-05 22:04:05,584 - root - [Epoch 0 Batch 330/1043] loss=4.9999, ppl=148.3954, gnorm=0.2902, throughput=29.79K wps, wc=61.37K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-05 22:04:07,569 - root - [Epoch 0 Batch 340/1043] loss=5.0215, ppl=151.6404, gnorm=0.2808, throughput=28.66K wps, wc=56.88K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-05 22:04:09,438 - root - [Epoch 0 Batch 350/1043] loss=4.8833, ppl=132.0599, gnorm=0.3249, throughput=29.38K wps, wc=54.86K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-05 22:04:11,596 - root - [Epoch 0 Batch 360/1043] loss=4.9959, ppl=147.8075, gnorm=0.2769, throughput=29.93K wps, wc=64.55K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-05 22:04:13,840 - root - [Epoch 0 Batch 370/1043] loss=4.8776, ppl=131.3112, gnorm=0.3714, throughput=29.85K wps, wc=66.97K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-05 22:04:15,682 - root - [Epoch 0 Batch 380/1043] loss=4.7877, ppl=120.0269, gnorm=0.3218, throughput=28.67K wps, wc=52.79K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-05 22:04:17,462 - root - [Epoch 0 Batch 390/1043] loss=4.7544, ppl=116.0908, gnorm=0.3316, throughput=28.65K wps, wc=50.94K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-05 22:04:19,122 - root - [Epoch 0 Batch 400/1043] loss=4.6091, ppl=100.3901, gnorm=0.3578, throughput=29.07K wps, wc=48.22K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-05 22:04:20,848 - root - [Epoch 0 Batch 410/1043] loss=4.7704, ppl=117.9686, gnorm=0.3023, throughput=27.98K wps, wc=48.27K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-05 22:04:22,812 - root - [Epoch 0 Batch 420/1043] loss=4.8054, ppl=122.1683, gnorm=0.2976, throughput=28.61K wps, wc=56.14K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-05 22:04:25,168 - root - [Epoch 0 Batch 430/1043] loss=4.8537, ppl=128.2162, gnorm=0.3029, throughput=29.44K wps, wc=69.33K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-05 22:04:27,422 - root - [Epoch 0 Batch 440/1043] loss=4.7528, ppl=115.9037, gnorm=0.3052, throughput=29.77K wps, wc=67.08K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-05 22:04:29,295 - root - [Epoch 0 Batch 450/1043] loss=4.6582, ppl=105.4495, gnorm=0.3361, throughput=28.67K wps, wc=53.68K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-05 22:04:31,013 - root - [Epoch 0 Batch 460/1043] loss=4.4439, ppl=85.1083, gnorm=0.3661, throughput=29.33K wps, wc=50.38K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-05 22:04:33,129 - root - [Epoch 0 Batch 470/1043] loss=4.7184, ppl=111.9923, gnorm=0.3061, throughput=28.71K wps, wc=60.70K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-05 22:04:35,058 - root - [Epoch 0 Batch 480/1043] loss=4.3705, ppl=79.0805, gnorm=0.3532, throughput=28.03K wps, wc=54.04K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-05 22:04:36,725 - root - [Epoch 0 Batch 490/1043] loss=4.4818, ppl=88.3892, gnorm=0.4521, throughput=27.81K wps, wc=46.32K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-05 22:04:38,469 - root - [Epoch 0 Batch 500/1043] loss=4.5235, ppl=92.1537, gnorm=0.3182, throughput=27.85K wps, wc=48.55K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-05 22:04:39,947 - root - [Epoch 0 Batch 510/1043] loss=4.3002, ppl=73.7157, gnorm=0.3437, throughput=28.16K wps, wc=41.62K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-05 22:04:41,390 - root - [Epoch 0 Batch 520/1043] loss=4.2065, ppl=67.1245, gnorm=0.3895, throughput=27.75K wps, wc=39.98K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-05 22:04:43,438 - root - [Epoch 0 Batch 530/1043] loss=4.6181, ppl=101.2972, gnorm=0.3040, throughput=28.61K wps, wc=58.58K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-05 22:04:45,249 - root - [Epoch 0 Batch 540/1043] loss=4.4807, ppl=88.2937, gnorm=0.3201, throughput=28.04K wps, wc=50.72K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-05 22:04:47,419 - root - [Epoch 0 Batch 550/1043] loss=4.5101, ppl=90.9332, gnorm=0.3302, throughput=29.02K wps, wc=62.95K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-05 22:04:49,061 - root - [Epoch 0 Batch 560/1043] loss=4.2907, ppl=73.0205, gnorm=0.3507, throughput=28.11K wps, wc=46.13K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-05 22:04:51,160 - root - [Epoch 0 Batch 570/1043] loss=4.3227, ppl=75.3938, gnorm=0.3194, throughput=29.14K wps, wc=61.12K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-05 22:04:53,126 - root - [Epoch 0 Batch 580/1043] loss=4.3229, ppl=75.4078, gnorm=0.3018, throughput=28.21K wps, wc=55.43K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-05 22:04:55,673 - root - [Epoch 0 Batch 590/1043] loss=4.5578, ppl=95.3729, gnorm=0.2665, throughput=29.03K wps, wc=73.93K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-05 22:04:57,688 - root - [Epoch 0 Batch 600/1043] loss=4.4568, ppl=86.2083, gnorm=0.2772, throughput=27.72K wps, wc=55.80K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-05 22:04:59,546 - root - [Epoch 0 Batch 610/1043] loss=4.2865, ppl=72.7086, gnorm=0.3502, throughput=28.15K wps, wc=52.28K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-05 22:05:02,045 - root - [Epoch 0 Batch 620/1043] loss=4.5301, ppl=92.7671, gnorm=0.2773, throughput=28.97K wps, wc=72.39K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-05 22:05:03,342 - root - [Epoch 0 Batch 630/1043] loss=4.0129, ppl=55.3092, gnorm=0.3310, throughput=26.59K wps, wc=34.44K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-05 22:05:05,380 - root - [Epoch 0 Batch 640/1043] loss=4.3137, ppl=74.7192, gnorm=0.4117, throughput=28.13K wps, wc=57.32K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-05 22:05:07,711 - root - [Epoch 0 Batch 650/1043] loss=4.4500, ppl=85.6263, gnorm=0.2950, throughput=28.51K wps, wc=66.42K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-05 22:05:09,345 - root - [Epoch 0 Batch 660/1043] loss=4.1775, ppl=65.2026, gnorm=0.3733, throughput=27.22K wps, wc=44.42K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-05 22:05:12,063 - root - [Epoch 0 Batch 670/1043] loss=4.5783, ppl=97.3454, gnorm=0.2769, throughput=28.60K wps, wc=77.68K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-05 22:05:14,218 - root - [Epoch 0 Batch 680/1043] loss=4.4025, ppl=81.6578, gnorm=0.2940, throughput=27.40K wps, wc=59.02K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-05 22:05:16,052 - root - [Epoch 0 Batch 690/1043] loss=4.1894, ppl=65.9830, gnorm=0.3172, throughput=27.57K wps, wc=50.54K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-05 22:05:17,983 - root - [Epoch 0 Batch 700/1043] loss=4.2774, ppl=72.0513, gnorm=0.3043, throughput=27.17K wps, wc=52.45K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-05 22:05:19,548 - root - [Epoch 0 Batch 710/1043] loss=4.1563, ppl=63.8353, gnorm=0.3532, throughput=26.42K wps, wc=41.32K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-05 22:05:21,389 - root - [Epoch 0 Batch 720/1043] loss=4.2351, ppl=69.0701, gnorm=0.3096, throughput=27.23K wps, wc=50.09K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-05 22:05:23,298 - root - [Epoch 0 Batch 730/1043] loss=4.1712, ppl=64.7908, gnorm=0.3094, throughput=27.26K wps, wc=52.02K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-05 22:05:25,494 - root - [Epoch 0 Batch 740/1043] loss=4.3100, ppl=74.4400, gnorm=0.2919, throughput=27.26K wps, wc=59.86K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-05 22:05:27,465 - root - [Epoch 0 Batch 750/1043] loss=4.1652, ppl=64.4067, gnorm=0.3143, throughput=27.09K wps, wc=53.35K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-05 22:05:29,922 - root - [Epoch 0 Batch 760/1043] loss=4.2704, ppl=71.5524, gnorm=0.2906, throughput=28.54K wps, wc=70.09K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-05 22:05:31,563 - root - [Epoch 0 Batch 770/1043] loss=4.0868, ppl=59.5473, gnorm=0.3133, throughput=26.36K wps, wc=43.21K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-05 22:05:34,255 - root - [Epoch 0 Batch 780/1043] loss=4.2848, ppl=72.5854, gnorm=0.2839, throughput=28.22K wps, wc=75.93K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-05 22:05:36,018 - root - [Epoch 0 Batch 790/1043] loss=4.1226, ppl=61.7192, gnorm=0.3077, throughput=26.56K wps, wc=46.81K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-05 22:05:38,188 - root - [Epoch 0 Batch 800/1043] loss=4.1815, ppl=65.4671, gnorm=0.3218, throughput=27.08K wps, wc=58.72K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-05 22:05:40,273 - root - [Epoch 0 Batch 810/1043] loss=4.0632, ppl=58.1578, gnorm=0.3073, throughput=27.53K wps, wc=57.38K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-05 22:05:42,400 - root - [Epoch 0 Batch 820/1043] loss=3.9488, ppl=51.8745, gnorm=0.3230, throughput=27.52K wps, wc=58.52K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-05 22:05:44,498 - root - [Epoch 0 Batch 830/1043] loss=4.1032, ppl=60.5355, gnorm=0.3281, throughput=27.31K wps, wc=57.24K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-05 22:05:46,423 - root - [Epoch 0 Batch 840/1043] loss=4.0580, ppl=57.8557, gnorm=0.3068, throughput=27.32K wps, wc=52.57K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-05 22:05:48,690 - root - [Epoch 0 Batch 850/1043] loss=4.1134, ppl=61.1517, gnorm=0.3005, throughput=28.31K wps, wc=64.14K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-05 22:05:50,687 - root - [Epoch 0 Batch 860/1043] loss=4.1108, ppl=60.9972, gnorm=0.2982, throughput=27.39K wps, wc=54.64K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-05 22:05:53,046 - root - [Epoch 0 Batch 870/1043] loss=4.1379, ppl=62.6711, gnorm=0.3149, throughput=27.91K wps, wc=65.81K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-05 22:05:54,912 - root - [Epoch 0 Batch 880/1043] loss=4.0776, ppl=59.0042, gnorm=0.3166, throughput=26.95K wps, wc=50.27K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-05 22:05:56,994 - root - [Epoch 0 Batch 890/1043] loss=4.1361, ppl=62.5599, gnorm=0.2971, throughput=26.97K wps, wc=56.11K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-05 22:05:59,199 - root - [Epoch 0 Batch 900/1043] loss=4.1666, ppl=64.4983, gnorm=0.2897, throughput=27.64K wps, wc=60.91K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-05 22:06:01,132 - root - [Epoch 0 Batch 910/1043] loss=3.9679, ppl=52.8745, gnorm=0.3130, throughput=26.74K wps, wc=51.65K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-05 22:06:03,318 - root - [Epoch 0 Batch 920/1043] loss=4.1545, ppl=63.7171, gnorm=0.2874, throughput=27.70K wps, wc=60.52K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-05 22:06:04,987 - root - [Epoch 0 Batch 930/1043] loss=3.9605, ppl=52.4814, gnorm=0.3001, throughput=26.08K wps, wc=43.51K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-05 22:06:06,811 - root - [Epoch 0 Batch 940/1043] loss=3.9206, ppl=50.4301, gnorm=0.3329, throughput=27.27K wps, wc=49.71K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-05 22:06:09,304 - root - [Epoch 0 Batch 950/1043] loss=4.1886, ppl=65.9326, gnorm=0.2828, throughput=28.46K wps, wc=70.92K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-05 22:06:11,919 - root - [Epoch 0 Batch 960/1043] loss=4.1424, ppl=62.9518, gnorm=0.2878, throughput=28.33K wps, wc=74.06K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-05 22:06:13,600 - root - [Epoch 0 Batch 970/1043] loss=3.8549, ppl=47.2223, gnorm=0.3436, throughput=26.21K wps, wc=44.03K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-05 22:06:16,186 - root - [Epoch 0 Batch 980/1043] loss=4.1688, ppl=64.6365, gnorm=0.2710, throughput=28.13K wps, wc=72.73K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-05 22:06:18,536 - root - [Epoch 0 Batch 990/1043] loss=4.0817, ppl=59.2452, gnorm=0.2949, throughput=27.84K wps, wc=65.39K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-05 22:06:20,648 - root - [Epoch 0 Batch 1000/1043] loss=3.9732, ppl=53.1566, gnorm=0.3070, throughput=26.37K wps, wc=55.65K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-05 22:06:23,036 - root - [Epoch 0 Batch 1010/1043] loss=4.0693, ppl=58.5149, gnorm=0.2771, throughput=27.50K wps, wc=65.61K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-05 22:06:24,736 - root - [Epoch 0 Batch 1020/1043] loss=3.8083, ppl=45.0741, gnorm=0.3299, throughput=26.94K wps, wc=45.72K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-05 22:06:26,813 - root - [Epoch 0 Batch 1030/1043] loss=3.9724, ppl=53.1106, gnorm=0.3136, throughput=26.98K wps, wc=55.97K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-05 22:06:28,752 - root - [Epoch 0 Batch 1040/1043] loss=3.9367, ppl=51.2513, gnorm=0.3453, throughput=27.49K wps, wc=53.27K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-05 22:06:52,743 - root - [Epoch 0] valid Loss=2.8493, valid ppl=17.2761, valid bleu=3.09\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-05 22:07:13,164 - root - [Epoch 0] test Loss=2.9845, test ppl=19.7764, test bleu=2.59\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-05 22:07:13,170 - root - Save best parameters to gnmt_en_vi_u512/valid_best.params\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-05 22:07:13,361 - root - Learning rate change to 0.0005\n"
     ]
    }
   ],
   "source": [
    "best_valid_bleu = 0.0\n",
    "\n",
    "# Run through each epoch\n",
    "for epoch_id in range(epochs):\n",
    "    log_avg_loss = 0\n",
    "    log_avg_gnorm = 0\n",
    "    log_wc = 0\n",
    "    log_start_time = time.time()\n",
    "\n",
    "    # Iterate through each batch\n",
    "    for batch_id, (src_seq, tgt_seq, src_valid_length, tgt_valid_length)\\\n",
    "            in enumerate(train_data_loader):\n",
    "\n",
    "        src_seq = src_seq.as_in_context(ctx)\n",
    "        tgt_seq = tgt_seq.as_in_context(ctx)\n",
    "        src_valid_length = src_valid_length.as_in_context(ctx)\n",
    "        tgt_valid_length = tgt_valid_length.as_in_context(ctx)\n",
    "\n",
    "        # Compute gradients and losses\n",
    "        with mx.autograd.record():\n",
    "            out, _ = model(src_seq, tgt_seq[:, :-1], src_valid_length, tgt_valid_length - 1)\n",
    "            loss = loss_function(out, tgt_seq[:, 1:], tgt_valid_length - 1).mean()\n",
    "            loss = loss * (tgt_seq.shape[1] - 1) / (tgt_valid_length - 1).mean()\n",
    "            loss.backward()\n",
    "\n",
    "        grads = [p.grad(ctx) for p in model.collect_params().values()]\n",
    "        gnorm = gluon.utils.clip_global_norm(grads, clip)\n",
    "        trainer.step(1)\n",
    "        src_wc = src_valid_length.sum().asscalar()\n",
    "        tgt_wc = (tgt_valid_length - 1).sum().asscalar()\n",
    "        step_loss = loss.asscalar()\n",
    "        log_avg_loss += step_loss\n",
    "        log_avg_gnorm += gnorm\n",
    "        log_wc += src_wc + tgt_wc\n",
    "        if (batch_id + 1) % log_interval == 0:\n",
    "            wps = log_wc / (time.time() - log_start_time)\n",
    "            logging.info('[Epoch {} Batch {}/{}] loss={:.4f}, ppl={:.4f}, gnorm={:.4f}, '\n",
    "                         'throughput={:.2f}K wps, wc={:.2f}K'\n",
    "                         .format(epoch_id, batch_id + 1, len(train_data_loader),\n",
    "                                 log_avg_loss / log_interval,\n",
    "                                 np.exp(log_avg_loss / log_interval),\n",
    "                                 log_avg_gnorm / log_interval,\n",
    "                                 wps / 1000, log_wc / 1000))\n",
    "            log_start_time = time.time()\n",
    "            log_avg_loss = 0\n",
    "            log_avg_gnorm = 0\n",
    "            log_wc = 0\n",
    "\n",
    "    # Evaluate the losses on validation and test datasets and find the corresponding BLEU score and log it\n",
    "    valid_loss, valid_translation_out = evaluate(val_data_loader)\n",
    "    valid_bleu_score, _, _, _, _ = nmt.bleu.compute_bleu([val_tgt_sentences], valid_translation_out)\n",
    "    logging.info('[Epoch {}] valid Loss={:.4f}, valid ppl={:.4f}, valid bleu={:.2f}'\n",
    "                 .format(epoch_id, valid_loss, np.exp(valid_loss), valid_bleu_score * 100))\n",
    "    test_loss, test_translation_out = evaluate(test_data_loader)\n",
    "    test_bleu_score, _, _, _, _ = nmt.bleu.compute_bleu([test_tgt_sentences], test_translation_out)\n",
    "    logging.info('[Epoch {}] test Loss={:.4f}, test ppl={:.4f}, test bleu={:.2f}'\n",
    "                 .format(epoch_id, test_loss, np.exp(test_loss), test_bleu_score * 100))\n",
    "\n",
    "    # Output the sentences we predicted on the validation and test datasets             \n",
    "    write_sentences(valid_translation_out,\n",
    "                    os.path.join(save_dir, 'epoch{:d}_valid_out.txt').format(epoch_id))\n",
    "    write_sentences(test_translation_out,\n",
    "                    os.path.join(save_dir, 'epoch{:d}_test_out.txt').format(epoch_id))\n",
    "\n",
    "    # Save the model if the BLEU score is better than the previous best\n",
    "    if valid_bleu_score > best_valid_bleu:\n",
    "        best_valid_bleu = valid_bleu_score\n",
    "        save_path = os.path.join(save_dir, 'valid_best.params')\n",
    "        logging.info('Save best parameters to {}'.format(save_path))\n",
    "        model.save_parameters(save_path)\n",
    "\n",
    "    # Update the learning rate based on the number of epochs that have passed\n",
    "    if epoch_id + 1 >= (epochs * 2) // 3:\n",
    "        new_lr = trainer.learning_rate * lr_update_factor\n",
    "        logging.info('Learning rate change to {}'.format(new_lr))\n",
    "        trainer.set_learning_rate(new_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "In this notebook, we have shown how to train a GNMT model on the IWSLT 2015 English-Vietnamese dataset using the Gluon NLP toolkit.\n",
    "The complete training script can be found [here](https://github.com/dmlc/gluon-nlp/blob/master/scripts/machine_translation/train_gnmt.py).\n",
    "The code sequence to reproduce the results can be seen on the [machine translation page](http://gluon-nlp.mxnet.io/model_zoo/machine_translation/index.html)."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}